---
layout: post
title: "ToyC Compiler"
date: 2025-06-04
description: "A minimal compiler in Python generating LLVM IR"
categories: [compilers, programming-languages]
tags: [compilers, programming-languages]
---

⚠️ This post is a work in progress. Updates coming soon!

# Introduction

ToyC is a minimal C-like language designed to show how source code becomes an executable program. In this post, you will learn how to scan, parse, lower to LLVM IR, and execute a program. If you are interested in the code, you can find it [here](https://github.com/eckertliam/toyc).

# Syntax of ToyC

The syntax of ToyC is a fusion between TypeScript, C, and Rust. It is designed to feel familiar and also be easy to parse. 

``` toyc

fn main(): i32 {
    let x: i32 = 1;
    let y: i32 = 2;
    return x + y;
}

```

We will only support basic types such as integers (`i32`, `i64`), floats (`f32`, `f64`), booleans (`bool`), and `void`. This allows us to cheat a bit and represent types as strings internally.

# Scanning

The first step in most compilers is to convert the source code into a stream of tokens. This is typically called scanning or lexing. Scanning breaks the source code down into meaningful units (tokens) and discards the rest making the next step easier.

Our tokens are defined in `tokens.py`. It's simple an enum class defining types of tokens, called `TokenKind`. Then the dataclass `Token` is used to represent a token, this contains the `kind`, `lexeme` (the actual text of the token), and `line`.

For ToyC, I have opted to use a simple greedy scanner. The Scanner class tracks some basic state:

``` python

class Scanner:
    def __init__(self, source: str) -> None:
        self.source = source
        self.line = 1
        self.start = 0
        self.current = 0

```

The scanner is driven by a few core methods `peek`, `advance`, and `scan_token`.

Peek is exactly what it sounds like. We don't always want to move the current pointer forward when we look at the next character. So we use `peek`:

``` python

def peek(self) -> str:
    if self.is_at_end():
        return "\0"
    return self.source[self.current]

```

Advance is the opposite of peek. It moves the current pointer forward and returns the character that was just moved past. It also updates the line number if the character is a newline.

``` python

def advance(self) -> str:
    char = self.source[self.current]
    self.current += 1
    if char == "\n":
        self.line += 1
    return char

```

Scan token is the workhorse of the scanner. It looks at the current character and decides what to do next. This is where scanner's get messy, `scan_token` is a large conditional dispatch function. Fortunately, we have a few helper methods to make it a bit more readable.

``` python

def scan_token(self) -> Token:
    self.start = self.current
    c = self.advance()

    if c == "(":
        return self.make_token(TokenKind.LPAREN)
    elif c == ")":
        return self.make_token(TokenKind.RPAREN)
    elif c == "{":
        return self.make_token(TokenKind.LBRACE)
    elif c == "}":
        return self.make_token(TokenKind.RBRACE)
    elif c == ";":
        return self.make_token(TokenKind.SEMICOLON)
    elif c == ":":
        return self.make_token(TokenKind.COLON)
    elif c == "+":
        return self.make_token(TokenKind.PLUS)
    elif c == "-":
        return self.make_token(TokenKind.MINUS)
    elif c == "*":
        return self.make_token(TokenKind.STAR)
    elif c == "/":
        return self.make_token(TokenKind.SLASH)
    elif c == "%":
        return self.make_token(TokenKind.MOD)
    elif c == "=":
        return self.make_token(TokenKind.EQ)
    elif c == "!":
        return self.make_token(TokenKind.BANG)
    elif c == "<":
        return self.make_token(TokenKind.LT)
    elif c == ">":
        return self.make_token(TokenKind.GT)
    elif c == ",":
        return self.make_token(TokenKind.COMMA)
    elif c == "&":
        if self.peek() == "&":
            self.advance()
            return self.make_token(TokenKind.AMPAMP)
        else:
            return self.error_token("Expected `&&`")
    elif c == "|":
        if self.peek() == "|":
            self.advance()
            return self.make_token(TokenKind.PIPEPIPE)
        else:
            return self.error_token("Expected `||`")
    elif c.isalpha() or c == "_":
        return self.identifier()
    elif c.isdigit():   
        return self.number()
    elif c in ["\n", " "]:
        return self.scan_token()
    else:
        return self.error_token(f"Unexpected character: {c}")

```

Messy? Absolutely. But it gets the job done—and for a small language like ToyC, that's all we need. Now you'll notice the helper methods. `make_token` grabs the lexeme `self.source[self.start:self.current]` and creates a `Token` object of the given kind. `error_token` is a simple wrapper that creates a `Token` of kind `ERROR` with the given message. `branch_token` is a helper that checks if the next character is a given character and returns a token of the given kind if it is, otherwise it returns a token of the else kind. I learned this trick from [Crafting Interpreters](https://craftinginterpreters.com/scanning-on-demand.html).

``` python

def branch_token(self, if_char: str, then_kind: TokenKind, else_kind: TokenKind) -> Token:
    if self.peek() == if_char:
        self.advance()
        return self.make_token(then_kind)
    else:
        return self.make_token(else_kind)

```

I have also added `__iter__` to the scanner so the scanner can be treated as an iterator. This is just syntactic sugar, but it helps us abstract away the details of the scanner during parsing.

``` python

def __iter__(self) -> Iterator[Token]:
    while not self.is_at_end():
        yield self.scan_token()
    yield Token(TokenKind.EOF, "", self.line)

```

If we fed a simple statement like `let x: i32 = 5;` to the scanner we would get the following tokens:

``` python

Token(kind=<TokenKind.LET: 5>, lexeme='let', line=1)
Token(kind=<TokenKind.IDENT: 2>, lexeme='x', line=1)
Token(kind=<TokenKind.COLON: 33>, lexeme=':', line=1)
Token(kind=<TokenKind.IDENT: 2>, lexeme='i32', line=1)
Token(kind=<TokenKind.EQ: 18>, lexeme='=', line=1)
Token(kind=<TokenKind.NUMBER: 1>, lexeme='5', line=1)
Token(kind=<TokenKind.SEMICOLON: 32>, lexeme=';', line=1)
Token(kind=<TokenKind.EOF: 35>, lexeme='', line=1)

```

Now that we have a stream of tokens, we can move on to the next step: parsing them into a structured representation of our program.